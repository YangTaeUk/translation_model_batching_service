# vLLM 동시처리 성능 테스트 결과 보고서

**생성일**: 2025-11-26 16:54:50
**테스트 환경**: NVIDIA RTX 3090 (24GB)
**모델**: google/gemma-3-12b-it (BitsAndBytes 4-bit 양자화)
**vLLM 버전**: 0.11.2

---

## 1. 테스트 결과 요약

### 1.1 핵심 발견사항

✅ **모든 테스트 레벨에서 100% 성공률 달성** (1~64 동시 요청)

**vLLM 서버 보고 용량**:
- KV Cache 메모리: **10.28 GiB**
- KV Cache 토큰: **28,064 tokens**
- 이론적 최대 동시처리: **8.62x** (4,096 토큰/요청 기준)

### 1.2 성능 메트릭

| 동시 요청 | 성공률 | TTFT (ms) | Latency (ms) | Throughput (tok/s) | 메모리 (MB) |
|----------|--------|-----------|--------------|-------------------|-------------|
| 1 | 100.0% | 84.0 | 1,818 | 55.6 | 23,398 |
| 2 | 100.0% | 106.4 | 7,089 | 14.2 | 23,398 |
| 4 | 100.0% | 140.9 | 7,192 | 14.0 | 23,398 |
| 8 | 100.0% | 181.8 | 7,360 | 13.7 | 23,398 |
| 16 | 100.0% | 227.9 | 7,633 | 13.2 | 23,398 |
| 32 | 100.0% | 352.6 | 8,106 | 12.5 | 23,398 |
| 48 | 100.0% | 375.0 | 9,926 | 10.2 | 23,398 |
| 64 | 100.0% | 439.7 | 10,459 | 9.7 | 23,398 |

### 1.3 한계점 분석

| 항목 | 값 | 비고 |
|------|-----|-----|
| 최대 안정 동시 요청 (100% 성공) | **64** | 테스트 범위 내 모두 성공 |
| 최대 테스트 동시 요청 | 64 | 추가 테스트 필요시 확장 가능 |
| 배치 처리 포화점 | 2+ | 2개 이상부터 배치 처리 활성화 |
| TTFT 증가율 | ~5.2x | 1개 → 64개 기준 |
| Latency 증가율 | ~5.8x | 1개 → 64개 기준 |

**특이사항**: vLLM의 continuous batching이 효과적으로 작동하여 2개 이상에서 latency가 급증하지만, 배치 내에서는 선형적으로 증가

---

## 2. RTX 3090 운영 권장사항

### 2.1 최대 용량 운영 (실측 기준)
- **최대 동시 요청**: 64개 (100% 성공률 확인)
- **평균 Latency**: ~10.5초 (64개 기준)
- **Throughput**: ~9.7 tokens/sec

### 2.2 SLA 기반 권장사항

| SLA 목표 | 권장 동시 요청 | 예상 Latency |
|---------|--------------|-------------|
| Latency < 8초 | 16개 이하 | ~7.6초 |
| Latency < 10초 | 48개 이하 | ~9.9초 |
| Latency < 12초 | 64개 이하 | ~10.5초 |
| 최대 Throughput | 1개 | 55.6 tok/s |

### 2.3 운영 시나리오별 권장

**번역 서비스 (낮은 부하)**:
- 동시 요청: **16~32개**
- 응답 시간: 7~8초
- 안정성: 매우 높음

**고부하 환경**:
- 동시 요청: **48~64개**
- 응답 시간: 10~11초
- 주의사항: 모니터링 강화 필요

---

## 3. 비양자화 Gemma 3 12B - 50개 동시요청 GPU 산정

### 3.1 메모리 요구량 계산

**Gemma 3 12B 비양자화 (bfloat16) 메모리 분석**:

| 구성 요소 | 계산 | 필요 메모리 |
|----------|------|------------|
| 모델 파라미터 | 12B × 2 bytes (bfloat16) | ~24 GB |
| KV Cache/요청 | (4096 토큰 × 48 레이어 × 2(K+V) × 4096 dim × 2 bytes) / 2^30 | ~1.5 GB |
| 시스템 오버헤드 | CUDA 컨텍스트 + 버퍼 | ~3 GB |

**50개 동시 요청 총 요구량**:
```
모델: 24 GB + KV Cache: 1.5 GB × 50 = 75 GB + 오버헤드: 3 GB
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
총 필요 VRAM: ~102 GB
```

### 3.2 GPU 옵션 비교

| GPU | VRAM | 단일 GPU 최대 동시 | 50개 처리 가능 | 필요 GPU 수 | 예상 비용 (USD) |
|-----|------|------------------|--------------|------------|----------------|
| RTX 3090 | 24GB | 0개 (모델 로드 불가) | ❌ | N/A | - |
| RTX 4090 | 24GB | 0개 (모델 로드 불가) | ❌ | N/A | - |
| A6000 | 48GB | ~14개 | ❌ | 4 (TP) | ~$18,000 |
| L40S | 48GB | ~14개 | ❌ | 4 (TP) | ~$16,000 |
| A100 40GB | 40GB | ~9개 | ❌ | 4 (TP) | ~$40,000 |
| A100 80GB | 80GB | ~35개 | ❌ | 2 (TP) | ~$30,000 |
| H100 80GB | 80GB | ~35개 | ❌ | 2 (TP) | ~$60,000 |
| **2× A100 80GB** | **160GB** | **~88개** | ✅ | **2 (TP=2)** | **~$30,000** |
| **2× H100 80GB** | **160GB** | **~88개** | ✅ | **2 (TP=2)** | **~$60,000** |

### 3.3 권장 구성

#### 옵션 A: 최소 비용 (On-Premise)
- **구성**: 2× NVIDIA A100 80GB (PCIe/SXM)
- **총 VRAM**: 160GB
- **최대 동시 요청**: ~88개 (목표 50개 충족)
- **예상 비용**: ~$30,000
- **vLLM 설정**: `--tensor-parallel-size 2`

#### 옵션 B: 최적 성능 (On-Premise)
- **구성**: 2× NVIDIA H100 80GB
- **총 VRAM**: 160GB
- **장점**: 3.35 TB/s 메모리 대역폭 (A100 대비 1.6배)
- **예상 비용**: ~$60,000
- **권장 사용처**: 저지연 실시간 서비스

#### 옵션 C: 클라우드 (AWS)
- **인스턴스**: p4d.24xlarge (8× A100 40GB)
- **총 VRAM**: 320GB
- **시간당 비용**: ~$32/hour
- **월 비용** (24/7): ~$23,000
- **장점**: 초기 투자 없음, 유연한 확장

#### 옵션 D: 양자화 유지 + 멀티 GPU
- **구성**: 2× RTX 3090 (BitsAndBytes 4-bit 양자화)
- **총 VRAM**: 48GB
- **최대 동시 요청**: ~20-30개 (추정)
- **예상 비용**: ~$3,000
- **제한사항**: 50개 동시 처리 불가, 품질 약간 저하

---

## 4. 결론

### 4.1 RTX 3090 + 양자화 환경 (현재 설정)

| 항목 | 결과 |
|------|------|
| 테스트 범위 | 1~64 동시 요청 |
| 성공률 | **100%** (모든 레벨) |
| 최대 안정 동시 요청 | **64개** |
| 권장 운영 동시 요청 | **16~32개** (SLA 기준) |
| 응답 시간 (32개 기준) | ~8초 |

**적합 사용처**:
- 낮은~중간 부하의 번역 서비스
- 배치 처리 워크로드
- 개발/테스트 환경

### 4.2 50개 동시요청 비양자화 환경

| 요구사항 | 권장 |
|---------|------|
| 최소 VRAM | ~102 GB |
| 최소 비용 옵션 | **2× A100 80GB** ($30,000) |
| 최적 성능 옵션 | **2× H100 80GB** ($60,000) |
| 클라우드 대안 | AWS p4d.24xlarge (~$32/hour) |

### 4.3 비용 대비 성능 분석

```
┌─────────────────────────────────────────────────────────────────┐
│  비용 효율성 순위 (50개 동시요청 기준)                              │
├─────────────────────────────────────────────────────────────────┤
│  1. 2× A100 80GB (On-Prem)     ~$30,000   ← 최소 비용            │
│  2. 클라우드 (단기 사용)         ~$32/hr    ← 유연성               │
│  3. 2× H100 80GB (On-Prem)     ~$60,000   ← 최고 성능            │
│  4. 양자화 + 3× RTX 3090       ~$4,500    ← 타협안 (제한적)       │
└─────────────────────────────────────────────────────────────────┘
```

---

## 5. 테스트 파일 위치

- 벤치마크 스크립트: `scripts/benchmark_concurrent.py`
- GPU 모니터: `scripts/monitor_gpu.py`
- 결과 분석: `scripts/analyze_results.py`
- 실행 스크립트: `scripts/run_all_tests.sh`
- 원본 결과: `results/all_concurrent_results.json`
- 차트: `results/charts/`

---

*이 보고서는 2025-11-26 vLLM 0.11.2 + Gemma 3 12B (4-bit 양자화) 벤치마크 테스트 결과입니다.*
