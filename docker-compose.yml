version: '3.8'

# =============================================================================
# Translation Model Batching Service - Local Development
# =============================================================================
# Usage:
#   1. Copy .env.example to .env: cp .env.example .env
#   2. Fill in your HuggingFace token and other settings
#   3. Run: docker-compose up -d
# =============================================================================

services:
  gemma-service:
    # Build from local Dockerfile
    build:
      context: .
      dockerfile: Dockerfile
    image: ${DOCKER_IMAGE:-gemma-3-service}:local
    container_name: ${CONTAINER_NAME:-gemma-3-server}
    restart: always
    runtime: nvidia

    environment:
      # HuggingFace token from .env file (NEVER hardcode!)
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
      - HF_HOME=/root/.cache/huggingface
      # Optional: Uncomment to bypass driver version check
      # - NVIDIA_DISABLE_REQUIRE=1

    volumes:
      # Mount host cache for model persistence
      - ${HF_CACHE_PATH:-/mnt/data2/hf_cache}:/root/.cache/huggingface

    ports:
      - "${VLLM_PORT:-8000}:8000"

    ipc: host

    # vLLM serve command with configurable parameters
    # RTX 3090 24GB: BitsAndBytes 4-bit quantization required for 12B model
    command: >
      ${MODEL_NAME:-google/gemma-3-12b-it}
      --dtype ${MODEL_DTYPE:-bfloat16}
      --quantization ${QUANTIZATION:-bitsandbytes}
      --load-format bitsandbytes
      --tensor-parallel-size ${TENSOR_PARALLEL_SIZE:-1}
      --gpu-memory-utilization ${GPU_MEMORY_UTILIZATION:-0.90}
      --max-model-len ${MAX_MODEL_LEN:-4096}
      --trust-remote-code

    # Health check for service monitoring
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s  # Model loading takes time

    # Logging configuration
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"
