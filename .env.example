# =============================================================================
# Translation Model Batching Service - Environment Variables
# =============================================================================
# Copy this file to .env and fill in your actual values:
#   cp .env.example .env
#
# IMPORTANT: Never commit .env to git!
# =============================================================================

# -----------------------------------------------------------------------------
# HuggingFace Configuration
# -----------------------------------------------------------------------------
# Get your token from: https://huggingface.co/settings/tokens
# Required for accessing gated models like Gemma
HUGGING_FACE_HUB_TOKEN=hf_your_token_here

# HuggingFace cache directory (optional, uses container default if not set)
# HF_HOME=/root/.cache/huggingface

# -----------------------------------------------------------------------------
# Model Configuration
# -----------------------------------------------------------------------------
# Model to serve (default: google/gemma-3-12b-it)
MODEL_NAME=google/gemma-3-12b-it

# Data type for model weights
# Options: bfloat16, float16, float32, auto
MODEL_DTYPE=bfloat16

# Quantization method (optional)
# Options: bitsandbytes, awq, gptq, squeezellm, None
QUANTIZATION=bitsandbytes

# Maximum model context length
MAX_MODEL_LEN=4096

# GPU memory utilization (0.0 - 1.0)
GPU_MEMORY_UTILIZATION=0.90

# Tensor parallel size (number of GPUs for model parallelism)
TENSOR_PARALLEL_SIZE=1

# -----------------------------------------------------------------------------
# Server Configuration
# -----------------------------------------------------------------------------
# Port to expose vLLM API
VLLM_PORT=8000

# Host binding (0.0.0.0 for all interfaces)
VLLM_HOST=0.0.0.0

# -----------------------------------------------------------------------------
# Docker Configuration
# -----------------------------------------------------------------------------
# Docker image name
DOCKER_IMAGE=gemma-3-service

# Docker container name
CONTAINER_NAME=gemma-3-server

# Host path for HuggingFace cache (for model persistence)
# Change this to your actual cache directory
HF_CACHE_PATH=/mnt/data2/hf_cache

# -----------------------------------------------------------------------------
# Cloud Deployment (for external cloud servers)
# -----------------------------------------------------------------------------
# DockerHub username for pulling images
DOCKERHUB_USERNAME=your_dockerhub_username

# External server configuration
# EXTERNAL_HOST=your-server.example.com
# EXTERNAL_PORT=8000
